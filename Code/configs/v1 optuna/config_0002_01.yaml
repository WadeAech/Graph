# GAT模型

dataset:
  name: Cora

model:
  # GAT通常使用ELU作为激活函数，效果比ReLU好
  activation: relu

  layers:
    # --- 第一层 (输入层 -> 隐藏层) ---
  - type: dropout
    dropout_rate: 0.6000000000000001
  - type: gat
    hidden_dim: 128
    heads: 8
  - type: elu

  - type: dropout
    dropout_rate: 0.6000000000000001
  - type: gat
    hidden_dim: 128
    heads: 8
  - type: elu

training:
  optimizer: 'adam'
  lr: 0.002243337906369562 # GAT通常需要比GCN更小的学习率
  wd: 0.009667740748678781 # 权重衰减
  epochs: 1000
  seeds: [42, 100, 2023]
